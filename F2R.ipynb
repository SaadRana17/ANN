{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "61Cx1vwd77mw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YQs5IhRY8Lb5",
        "outputId": "0959671e-55c3-451c-c3a0-1a167341e942"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ls\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BifXbGix8OXJ",
        "outputId": "4caf2d9f-c953-462a-b0d8-efdc6f16971e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[0m\u001b[01;34mgdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd Data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RNK8p8Pu8P0G",
        "outputId": "76d861f7-d25f-4987-d37f-c3f9a54a60ab"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'Data'\n",
            "/content\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam"
      ],
      "metadata": {
        "id": "ZQi0f8Hb86Cv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "1J-WAqLg88GP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "-c7IB63Z7Pw1"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# Define the path to the data folder\n",
        "data_folder = \"/content/gdrive/MyDrive/Saad/Data\"\n",
        "\n",
        "# Define the image dimensions\n",
        "input_height = 256\n",
        "input_width = 256\n",
        "input_channels = 1\n",
        "\n",
        "# Load and preprocess the dataset\n",
        "def load_dataset(folder):\n",
        "    file_paths = glob.glob(os.path.join(folder, \"*.png\"))  # Assuming PNG format, modify if necessary\n",
        "    dataset = []\n",
        "\n",
        "    for file_path in file_paths:\n",
        "        # Load and preprocess each image\n",
        "        image = Image.open(file_path).convert(\"L\")  # Load as grayscale image\n",
        "        image = image.resize((input_width, input_height), Image.BICUBIC)  # Resize to desired dimensions\n",
        "        image = np.array(image)  # Convert to numpy array\n",
        "        image = np.expand_dims(image, axis=-1)  # Add channel dimension\n",
        "        image = (image - 127.5) / 127.5  # Normalize to range [-1, 1]\n",
        "\n",
        "        dataset.append(image)\n",
        "\n",
        "    return np.array(dataset)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the real depth maps and simulated depth maps\n",
        "real_depth_dataset = load_dataset(os.path.join(data_folder, \"Real_Depth\"))\n",
        "simulated_depth_dataset = load_dataset(os.path.join(data_folder, \"Virtual/livingroom1-depth-clean/\"))\n",
        "\n",
        "# Shuffle the datasets\n",
        "random.shuffle(real_depth_dataset)\n",
        "random.shuffle(simulated_depth_dataset)\n",
        "\n",
        "# Create TensorFlow Datasets\n",
        "real_depth_dataset = tf.data.Dataset.from_tensor_slices(real_depth_dataset)\n",
        "simulated_depth_dataset = tf.data.Dataset.from_tensor_slices(simulated_depth_dataset)\n",
        "\n",
        "# Combine the datasets\n",
        "batch_size=10\n",
        "dataset = tf.data.Dataset.zip((real_depth_dataset, simulated_depth_dataset))\n",
        "dataset = dataset.shuffle(buffer_size=1000).batch(batch_size)"
      ],
      "metadata": {
        "id": "8K2xoM1O8svh"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_generator(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    # Downsampling\n",
        "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    # Upsampling\n",
        "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.ReLU()(x)\n",
        "\n",
        "    # Output\n",
        "    x = layers.Conv2D(1, kernel_size=7, strides=1, padding='same', activation='tanh')(x)\n",
        "\n",
        "    model = keras.models.Model(inputs, x)\n",
        "    return model\n",
        "\n",
        "\n",
        "def build_discriminator(input_shape):\n",
        "    inputs = layers.Input(shape=input_shape)\n",
        "\n",
        "    x = layers.Conv2D(64, kernel_size=4, strides=2, padding='same')(inputs)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(128, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(256, kernel_size=4, strides=2, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(512, kernel_size=4, strides=1, padding='same')(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "    x = layers.LeakyReLU(alpha=0.2)(x)\n",
        "\n",
        "    x = layers.Conv2D(1, kernel_size=4, strides=1, padding='same')(x)\n",
        "\n",
        "    model = keras.models.Model(inputs, x)\n",
        "    return model\n"
      ],
      "metadata": {
        "id": "peWCzPYa9ncz"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "\n",
        "# Define the generator and discriminator models\n",
        "#def build_generator(input_shape):\n",
        "    # Implement the generator architecture\n",
        "    # ...\n",
        "\n",
        "#def build_discriminator(input_shape):\n",
        "    # Implement the discriminator architecture\n",
        "    # ...\n",
        "\n",
        "# Build the generator and discriminator models\n",
        "input_shape = (input_height, input_width, input_channels)\n",
        "generator_A2B = build_generator(input_shape)\n",
        "generator_B2A = build_generator(input_shape)\n",
        "discriminator_A = build_discriminator(input_shape)\n",
        "discriminator_B = build_discriminator(input_shape)\n",
        "\n",
        "# Define the loss functions\n",
        "loss_fn = keras.losses.BinaryCrossentropy(from_logits=True)\n",
        "\n",
        "# Define the identity loss\n",
        "def identity_loss(real_image, same_image):\n",
        "    loss = tf.reduce_mean(tf.abs(real_image - same_image))\n",
        "    return loss\n",
        "\n",
        "# Define the cycle consistency loss\n",
        "def cycle_loss(real_image, cycled_image):\n",
        "    loss = tf.reduce_mean(tf.abs(real_image - cycled_image))\n",
        "    return loss\n",
        "\n",
        "# Define the adversarial loss\n",
        "def adversarial_loss(real_image, generated_image):\n",
        "    disc_generated_output = discriminator(generated_image)\n",
        "    loss = loss_fn(tf.ones_like(disc_generated_output), disc_generated_output)\n",
        "    return loss\n",
        "\n",
        "# Define the generator and discriminator optimizers\n",
        "generator_optimizer = Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "discriminator_optimizer = Adam(learning_rate=2e-4, beta_1=0.5)\n",
        "\n",
        "# Define the training loop\n",
        "@tf.function\n",
        "def train_step(real_depth, real_simulated):\n",
        "    with tf.GradientTape(persistent=True) as tape:\n",
        "        # Generate simulated depth maps using real depth maps\n",
        "        fake_simulated = generator_A2B(real_depth)\n",
        "        # Generate real depth maps using simulated depth maps\n",
        "        cycled_depth = generator_B2A(real_simulated)\n",
        "        \n",
        "        # Identity mapping\n",
        "        same_depth = generator_B2A(real_depth)\n",
        "        same_simulated = generator_A2B(real_simulated)\n",
        "        \n",
        "        # Compute the generator losses\n",
        "        gen_A2B_loss = adversarial_loss(discriminator_B(fake_simulated))\n",
        "        gen_B2A_loss = adversarial_loss(discriminator_A(cycled_depth))\n",
        "        total_gen_loss = gen_A2B_loss + gen_B2A_loss + identity_loss(real_depth, same_depth) + identity_loss(real_simulated, same_simulated)\n",
        "        \n",
        "        # Compute the discriminator losses\n",
        "        disc_A_loss = adversarial_loss(discriminator_A(real_depth), discriminator_A(fake_simulated))\n",
        "        disc_B_loss = adversarial_loss(discriminator_B(real_simulated), discriminator_B(cycled_depth))\n",
        "        \n",
        "    # Calculate the gradients for generator and discriminator\n",
        "    generator_gradients = tape.gradient(total_gen_loss, generator_A2B.trainable_variables + generator_B2A.trainable_variables)\n",
        "    discriminator_gradients = tape.gradient(disc_A_loss, discriminator_A.trainable_variables) + tape.gradient(disc_B_loss, discriminator_B.trainable_variables)\n",
        "    \n",
        "    # Apply the gradients to the optimizer\n",
        "    generator_optimizer.apply_gradients(zip(generator_gradients, generator_A2B.trainable_variables + generator_B2A.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(discriminator_gradients, discriminator_A.trainable_variables + discriminator_B.trainable_variables))\n",
        "\n",
        "# Start the training loop\n",
        "epochs = 10\n",
        "batch_size = 10\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Iterate over the batches of the dataset\n",
        "    for batch in dataset:\n",
        "        real_depth, real_simulated = batch\n",
        "        train_step(real_depth, real_simulated)\n",
        "        \n",
        "    # Print the training progress after each epoch\n",
        "    print(\"Epoch {}/{}\".format(epoch+1, epochs))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Save the trained generator models\n",
        "generator_A2B.save('generator_A2B.h5')\n",
        "generator_B2A.save('generator_B2A.h5')# Example usage: Generate simulated depth maps from real depth maps\n",
        "#fake_simulated = generator_A2B(real_depth)\n",
        "\n",
        "# Example usage: Generate real depth maps from simulated depth maps\n",
        "#cycled_depth = generator_B2A(real_simulated)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "anF0sEoe92dy",
        "outputId": "3009babd-06ab-4912-8f5b-9ca610e5e541"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e2d2d33cec47>\u001b[0m in \u001b[0;36m<cell line: 18>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;31m# Build the generator and discriminator models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0minput_shape\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_height\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mgenerator_A2B\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mgenerator_B2A\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'input_height' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the accuracy metric\n",
        "accuracy = keras.metrics.BinaryAccuracy()\n",
        "\n",
        "# Start the training loop\n",
        "epochs = 100\n",
        "batch_size = 16\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Reset the accuracy metric at the beginning of each epoch\n",
        "    accuracy.reset_states()\n",
        "\n",
        "    # Iterate over the batches of the dataset\n",
        "    for batch in dataset:\n",
        "        real_depth, real_simulated = batch\n",
        "        train_step(real_depth, real_simulated)\n",
        "\n",
        "        # Calculate training accuracy\n",
        "        fake_simulated = generator_A2B(real_depth)\n",
        "        accuracy.update_state(tf.ones_like(fake_simulated), fake_simulated)\n",
        "\n",
        "    # Compute training accuracy at the end of each epoch\n",
        "    training_acc = accuracy.result()\n",
        "\n",
        "    # Reset the accuracy metric for validation\n",
        "    accuracy.reset_states()\n",
        "\n",
        "\n",
        "\n",
        "    # Compute validation accuracy at the end of each epoch\n",
        "    validation_acc = accuracy.result()\n",
        "\n",
        "    # Print the training and validation accuracies after each epoch\n",
        "    print(\"Epoch {}/{} - Training Accuracy: {:.4f} - Validation Accuracy: {:.4f}\".format(epoch + 1, epochs, training_acc, validation_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JJshWUrcAMt1",
        "outputId": "e429c179-77b6-45f7-e3a8-cf2018942504"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 2/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 3/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 4/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 5/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 6/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 7/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 8/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 9/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 10/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 11/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 12/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 13/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 14/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 15/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 16/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 17/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 18/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 19/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 20/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 21/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 22/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 23/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 24/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 25/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 26/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 27/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 28/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 29/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 30/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 31/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 32/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 33/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 34/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 35/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 36/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 37/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 38/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 39/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 40/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 41/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 42/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 43/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 44/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 45/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 46/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 47/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 48/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 49/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 50/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 51/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 52/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 53/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 54/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 55/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 56/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 57/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 58/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 59/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 60/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 61/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 62/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 63/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 64/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 65/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 66/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 67/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 68/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 69/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 70/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 71/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 72/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 73/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 74/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 75/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 76/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 77/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 78/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 79/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 80/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 81/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 82/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 83/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 84/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 85/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 86/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 87/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 88/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 89/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 90/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 91/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 92/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 93/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 94/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 95/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 96/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 97/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 98/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 99/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n",
            "Epoch 100/100 - Training Accuracy: 0.0000 - Validation Accuracy: 0.0000\n"
          ]
        }
      ]
    }
  ]
}